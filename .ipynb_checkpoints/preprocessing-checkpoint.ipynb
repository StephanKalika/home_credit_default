{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "gc.enable()\n",
    "path = \"/Users/danielsaxton/home_credit_default_risk/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_agg_func(g):\n",
    "    mask12 = g[\"DAYS_DECISION\"] >= -360\n",
    "    mask24 = g[\"DAYS_DECISION\"] >= -720\n",
    "\n",
    "    d = {\"AVG_SYNTH_TARGET_12M\": np.nanmean(g[\"SYNTHETIC_TARGET\"].where(mask12)), \n",
    "        \"AVG_PREV_PROP_APPROVED_12M\": np.nanmean((g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]).where(mask12)), \n",
    "        \"AVG_PREV_PROP_APPROVED_24M\": np.nanmean((g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]).where(mask24)), \n",
    "        \"MAX_PREV_PROP_APPROVED_12M\": np.nanmax((g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]).where(mask12)), \n",
    "        \"MAX_PREV_PROP_APPROVED_24M\": np.nanmax((g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]).where(mask24)), \n",
    "        \"COUNT_PREV_APP\": len(g), \n",
    "        \"MIN_PREV_DAYS_TERMINATION\": np.nanmin(g[\"DAYS_TERMINATION\"]), \n",
    "        \"MAX_PREV_DAYS_TERMINATION\": np.nanmax(g[\"DAYS_TERMINATION\"]), \n",
    "        \"AVG_PREV_DAYS_TERMINATION\": np.nanmean(g[\"DAYS_TERMINATION\"]), \n",
    "        \"RANGE_PREV_DAYS_TERMINATION\": np.nanmax(g[\"DAYS_TERMINATION\"]) - np.nanmin(g[\"DAYS_TERMINATION\"]),  \n",
    "        \"MIN_PREV_AMT_CREDIT\": np.nanmin(g[\"AMT_CREDIT\"]),\n",
    "        \"MAX_PREV_AMT_CREDIT\": np.nanmax(g[\"AMT_CREDIT\"]),\n",
    "        \"AVG_PREV_AMT_CREDIT\": np.nanmean(g[\"AMT_CREDIT\"]),\n",
    "        \"MIN_PREV_AMT_CREDIT_WEIGHTED\": np.nanmin(g[\"AMT_CREDIT\"] / abs(g[\"DAYS_DECISION\"])),\n",
    "        \"MAX_PREV_AMT_CREDIT_WEIGHTED\": np.nanmax(g[\"AMT_CREDIT\"] / abs(g[\"DAYS_DECISION\"])),\n",
    "        \"AVG_PREV_AMT_CREDIT_WEIGHTED\": np.nanmean(g[\"AMT_CREDIT\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"MIN_PREV_AMT_CREDIT_DIV_ANNUITY\": np.nanmin(g[\"AMT_CREDIT\"] / g[\"AMT_ANNUITY\"]),\n",
    "        \"MAX_PREV_AMT_CREDIT_DIV_ANNUITY\": np.nanmax(g[\"AMT_CREDIT\"] / g[\"AMT_ANNUITY\"]),\n",
    "        \"AVG_PREV_AMT_CREDIT_DIV_ANNUITY\": np.nanmean(g[\"AMT_CREDIT\"] / g[\"AMT_ANNUITY\"]),\n",
    "        \"MIN_PREV_AMT_CREDIT_DIV_ANNUITY_WEIGHTED\": np.nanmin(g[\"AMT_CREDIT\"] / g[\"AMT_ANNUITY\"] / abs(g[\"DAYS_DECISION\"])),\n",
    "        \"MAX_PREV_AMT_CREDIT_DIV_ANNUITY_WEIGHTED\": np.nanmax(g[\"AMT_CREDIT\"] / g[\"AMT_ANNUITY\"] / abs(g[\"DAYS_DECISION\"])),\n",
    "        \"AVG_PREV_AMT_CREDIT_DIV_ANNUITY_WEIGHTED\": np.nanmean(g[\"AMT_CREDIT\"] / g[\"AMT_ANNUITY\"] / abs(g[\"DAYS_DECISION\"])),  \n",
    "        \"MAX_PREV_AMT_ANNUITY\": np.nanmin(g[\"AMT_ANNUITY\"]), \n",
    "        \"AVG_PREV_AMT_ANNUITY\": np.nanmean(g[\"AMT_ANNUITY\"]), \n",
    "        \"MAX_PREV_AMT_ANNUITY_WEIGHTED\": np.nanmin(g[\"AMT_ANNUITY\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"AVG_PREV_AMT_ANNUITY_WEIGHTED\": np.nanmean(g[\"AMT_ANNUITY\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"MIN_DAYS_DECISION\": np.nanmin(g[\"DAYS_DECISION\"]), \n",
    "        \"MAX_DAYS_DECISION\": np.nanmax(g[\"DAYS_DECISION\"]), \n",
    "        \"RANGE_DAYS_DECISION\": np.nanmax(g[\"DAYS_DECISION\"]) - np.nanmin(g[\"DAYS_DECISION\"]),\n",
    "        \"SUM_DAYS_LAST_DUE_NULL\": np.nansum(g[\"DAYS_LAST_DUE\"].isnull()), \n",
    "        \"AVG_DAYS_LAST_DUE_NULL\": np.nanmean(g[\"DAYS_LAST_DUE\"].isnull()), \n",
    "        \"AVG_PREV_REQ_AMOUNT_WEIGHTED\": np.nanmean(g[\"AMT_APPLICATION\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"MAX_PREV_REQ_AMOUNT_WEIGHTED\": np.nanmax(g[\"AMT_APPLICATION\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"AVG_PREV_REQ_AMOUNT\": np.nanmean(g[\"AMT_APPLICATION\"]), \n",
    "        \"MAX_PREV_REQ_AMOUNT\": np.nanmax(g[\"AMT_APPLICATION\"]), \n",
    "        \"AVG_PREV_RATE_DOWNPAYMENT_WEIGHTED\": np.nanmean(g[\"RATE_DOWN_PAYMENT\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"AVG_PREV_PROP_APPROVED_WEIGHTED\": np.nanmean(g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"MAX_PREV_PROP_APPROVED_WEIGHTED\": np.nanmax(g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"AVG_PREV_RATE_DOWNPAYMENT\": np.nanmean(g[\"RATE_DOWN_PAYMENT\"]), \n",
    "        \"AVG_PREV_PROP_APPROVED\": np.nanmean(g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]), \n",
    "        \"MAX_PREV_PROP_APPROVED\": np.nanmax(g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]), \n",
    "        \"MIN_PREV_PROP_APPROVED\": np.nanmin(g[\"AMT_CREDIT\"] / g[\"AMT_APPLICATION\"]), \n",
    "        \"AVG_PREV_REQ_AMOUNT\": np.nanmean(g[\"AMT_APPLICATION\"]), \n",
    "        \"MAX_PREV_REQ_AMOUNT\": np.nanmax(g[\"AMT_APPLICATION\"]), \n",
    "        \"AVG_PREV_RATE_DOWNPAYMENT\": np.nanmean(g[\"RATE_DOWN_PAYMENT\"]), \n",
    "        \"AVG_PREV_INT_RATE\": np.nanmean(g[\"RATE_INTEREST_PRIMARY\"]), \n",
    "        \"SUM_PREV_URGENT_NEEDS\": np.nansum(g[\"NAME_CASH_LOAN_PURPOSE\"] == \"Urgent needs\"), \n",
    "        \"SUM_PREV_REPAIRS\": np.nansum(g[\"NAME_CASH_LOAN_PURPOSE\"] == \"Repairs\"), \n",
    "        \"SUM_PREV_OTHER\": np.nansum(g[\"NAME_CASH_LOAN_PURPOSE\"] == \"Other\"), \n",
    "        \"SUM_PREV_LIMIT_REJECT\": np.nansum(g[\"CODE_REJECT_REASON\"] == \"LIMIT\"), \n",
    "        \"SUM_REFUSED_CONTRACT\": np.nansum(g[\"NAME_CONTRACT_STATUS\"] == \"Refused\"), \n",
    "        \"SUM_CANC_CONTRACT\": np.nansum(g[\"NAME_CONTRACT_STATUS\"] == \"Canceled\"), \n",
    "        \"SUM_APPR_CONTRACT\": np.nansum(g[\"NAME_CONTRACT_STATUS\"] == \"Approved\"), \n",
    "        \"SUM_PREV_HC_REJECT\": np.nansum(g[\"CODE_REJECT_REASON\"] == \"HC\"), \n",
    "        \"SUM_PREV_INSURE_REQ\": np.nansum(g[\"NFLAG_INSURED_ON_APPROVAL\"]), \n",
    "        \"COUNT_PREV_WALK_IN\": np.nansum(g[\"NAME_PRODUCT_TYPE\"] == \"walk-in\"), \n",
    "        \"COUNT_PREV_HIGH_YIELD\": np.nansum(g[\"NAME_YIELD_GROUP\"] == \"high\"), \n",
    "        \"COUNT_PREV_LOW_YIELD\": np.nansum(g[\"NAME_YIELD_GROUP\"].apply(lambda x: x.startswith(\"low\"))), \n",
    "        \"AVG_SYNTH_TARGET\": np.nanmean(g[\"SYNTHETIC_TARGET\"]), \n",
    "        \"SUM_SYNTH_TARGET_WEIGHTED\": np.nansum(g[\"SYNTHETIC_TARGET\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"SUM_SYNTH_TARGET\": np.nansum(g[\"SYNTHETIC_TARGET\"]), \n",
    "        \"MAX_SYNTH_TARGET\": np.nanmax(g[\"SYNTHETIC_TARGET\"]), \n",
    "        \"MIN_SYNTH_TARGET\": np.nanmin(g[\"SYNTHETIC_TARGET\"]), \n",
    "        \"RANGE_SYNTH_TARGET\": np.nanmax(g[\"SYNTHETIC_TARGET\"]) - np.min(g[\"SYNTHETIC_TARGET\"]), \n",
    "        \"SUM_DAYS_LAST_DUE_1ST_VERSION_EQ_DAYS_LAST_DUE\": np.nansum(g[\"DAYS_LAST_DUE_1ST_VERSION\"] == g[\"DAYS_LAST_DUE\"]), \n",
    "        \"SUM_DAYS_FIRST_DRAWING_SENTINEL\": np.nansum(g[\"DAYS_FIRST_DRAWING_SENTINEL\"]), \n",
    "        \"SUM_DAYS_FIRST_DRAWING_SENTINEL_WEIGHTED\": np.nansum(g[\"DAYS_FIRST_DRAWING_SENTINEL\"] / abs(g[\"DAYS_DECISION\"])), \n",
    "        \"MAX_DAYS_FIRST_DRAWING_SENTINEL_WEIGHTED\": np.nanmax(g[\"DAYS_FIRST_DRAWING_SENTINEL\"] / abs(g[\"DAYS_DECISION\"])),         \n",
    "        \"SUM_DAYS_LAST_DUE_LT_FIRST_VERSION\": np.nansum(g[\"DAYS_LAST_DUE\"] < g[\"DAYS_LAST_DUE_1ST_VERSION\"]), \n",
    "        \"MAX_DAYS_FIRST_DRAWING_DAYS_DUE_SENTINEL\": np.nanmax(g[\"DAYS_FIRST_DRAWING_SENTINEL\"] * g[\"DAYS_FIRST_DUE_SENTINEL\"]), \n",
    "        \"MAX_DAYS_FIRST_SENTINEL_COMP_DAYS_LAST_SENTINEL\": np.nansum((1 - g[\"DAYS_FIRST_DUE_SENTINEL\"]) * g[\"DAYS_LAST_DUE_1ST_VERSION_SENTINEL\"])}\n",
    "\n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_application = pd.read_csv(path + \"previous_application.csv\")\n",
    "\n",
    "with open(path + \"linear_model.pkl\", \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "impute = Imputer(strategy=\"median\")\n",
    "scale = StandardScaler()\n",
    "\n",
    "cols = [\"AMT_ANNUITY\", \n",
    "        \"AMT_CREDIT\", \n",
    "        \"AMT_GOODS_PRICE\", \n",
    "        \"HOUR_APPR_PROCESS_START\", \n",
    "        \"NAME_CONTRACT_TYPE\", \n",
    "        \"NAME_TYPE_SUITE\", \n",
    "        \"WEEKDAY_APPR_PROCESS_START\"]\n",
    "\n",
    "prev_temp = pd.get_dummies(previous_application[cols])\n",
    "\n",
    "dummy_cols = [\"AMT_CREDIT\",\n",
    "              \"AMT_GOODS_PRICE\",\n",
    "              \"HOUR_APPR_PROCESS_START\",\n",
    "              \"NAME_CONTRACT_TYPE_Cash loans\",\n",
    "              \"NAME_CONTRACT_TYPE_Revolving loans\",\n",
    "              \"NAME_TYPE_SUITE_Children\",\n",
    "              \"NAME_TYPE_SUITE_Family\",\n",
    "              \"NAME_TYPE_SUITE_Group of people\",\n",
    "              \"NAME_TYPE_SUITE_Other_A\",\n",
    "              \"NAME_TYPE_SUITE_Other_B\",\n",
    "              \"NAME_TYPE_SUITE_Spouse, partner\",\n",
    "              \"NAME_TYPE_SUITE_Unaccompanied\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_FRIDAY\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_MONDAY\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_SATURDAY\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_SUNDAY\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_THURSDAY\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_TUESDAY\",\n",
    "              \"WEEKDAY_APPR_PROCESS_START_WEDNESDAY\"]\n",
    "\n",
    "previous_application[\"SYNTHETIC_TARGET\"] = clf.predict_proba(scale.fit_transform(impute.fit_transform(prev_temp[dummy_cols])))[:,1]\n",
    "previous_application[\"DAYS_FIRST_DRAWING_SENTINEL\"] = (previous_application[\"DAYS_FIRST_DRAWING\"] == 365243).astype(int)\n",
    "previous_application[\"DAYS_FIRST_DUE_SENTINEL\"] = (previous_application[\"DAYS_FIRST_DUE\"] == 365243).astype(int)\n",
    "previous_application[\"DAYS_LAST_DUE_1ST_VERSION_SENTINEL\"] = (previous_application[\"DAYS_LAST_DUE_1ST_VERSION\"] == 365243).astype(int)\n",
    "previous_application[\"DAYS_LAST_DUE_SENTINEL\"] = (previous_application[\"DAYS_LAST_DUE\"] == 365243).astype(int)\n",
    "previous_application[\"DAYS_TERMINATION_SENTINEL\"] = (previous_application[\"DAYS_TERMINATION\"] == 365243).astype(int)\n",
    "\n",
    "previous_agg = previous_application.groupby(\"SK_ID_CURR\").apply(previous_agg_func).reset_index()\n",
    "\n",
    "previous_agg.to_csv(path + \"previous_agg.csv\", index=False, header=True)\n",
    "del prev_temp, previous_application, previous_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bureau Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_balance_agg_func(g):\n",
    "    mask6 = g[\"MONTHS_BALANCE\"] >= -6\n",
    "    mask12 = g[\"MONTHS_BALANCE\"] >= -12\n",
    "    closed = g[\"STATUS\"] == \"C\"\n",
    "\n",
    "    d = {\"WORST_DQ_BUREAU_BALANCE_6M\": np.nanmax(g[\"STATUS\"].apply(lambda x: 0 if x == \"C\" else int(x)).where(mask6)), \n",
    "        \"WORST_DQ_BUREAU_BALANCE_12M\": np.nanmax(g[\"STATUS\"].apply(lambda x: 0 if x == \"C\" else int(x)).where(mask12)), \n",
    "        \n",
    "        \"LEN_BUREAU_BALANCE\": np.nansum(~closed), \n",
    "        \"SUM_CLOSED_BUREAU_BALANCE\": np.nansum(closed), \n",
    "        \"SUM_CURRENT_BUREAU_BALANCE\": np.nansum(g[\"STATUS\"] == \"0\"), \n",
    "        \"SUM_DQ_BUREAU_BALANCE\": np.nansum(g[\"STATUS\"].isin([\"1\", \"2\", \"3\", \"3\", \"4\", \"5\"])),\n",
    "        \"WORST_DQ_BUREAU_BALANCE\": np.nanmax(g[\"STATUS\"].apply(lambda x: 0 if x == \"C\" else int(x))), \n",
    "        \"AVG_MONTHS_BALANCE_BUREAU_BALANCE\": np.nansum(abs(g[\"MONTHS_BALANCE\"]).where(~closed)) / np.nansum(~closed), \n",
    "        \"MIN_MONTHS_BALANCE_BUREAU_BALANCE\": np.nanmin(g[\"MONTHS_BALANCE\"].where(~closed)), \n",
    "        \"MAX_MONTHS_BALANCE_BUREAU_BALANCE\": np.nanmax(g[\"MONTHS_BALANCE\"].where(~closed))}\n",
    "\n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv(path + \"bureau_balance.csv\")\n",
    "\n",
    "bureau_balance[\"STATUS\"] = bureau_balance[\"STATUS\"].where(lambda x: x != \"X\").fillna(\"0\")\n",
    "\n",
    "bureau_balance_agg = bureau_balance.groupby(\"SK_ID_BUREAU\").apply(bureau_balance_agg_func).reset_index()\n",
    "bureau_balance_agg.to_csv(path + \"bureau_balance_agg.csv\", index=False, header=True)\n",
    "del bureau_balance, bureau_balance_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bureau\n",
    "\n",
    "This is dependent on `bureau_balance_agg` above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_agg_func(g):\n",
    "    mask6 = g[\"DAYS_CREDIT\"] >= -180\n",
    "    mask12 = g[\"DAYS_CREDIT\"] >= -360\n",
    "    \n",
    "    d = {\"MAX_WORST_DQ_BUREAU_BALANCE_6M\": np.nanmax(g[\"WORST_DQ_BUREAU_BALANCE_6M\"].where(mask6)), \n",
    "        \"MAX_WORST_DQ_BUREAU_BALANCE_12M\": np.nanmax(g[\"WORST_DQ_BUREAU_BALANCE_12M\"].where(mask12)), \n",
    "        \"MAX_BUREAU_UTILIZATION_6M\": np.nanmax((g[\"AMT_CREDIT_SUM_DEBT\"] / g[\"AMT_CREDIT_SUM_LIMIT\"]).where(mask6)), \n",
    "        \"MAX_BUREAU_UTILIZATION_12M\": np.nanmax((g[\"AMT_CREDIT_SUM_DEBT\"] / g[\"AMT_CREDIT_SUM\"]).where(mask12)), \n",
    "        \n",
    "         \n",
    "        \"AVG_LEN_BUREAU_BALANCE\": np.nanmean(g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"SUM_LEN_BUREAU_BALANCE\": np.nansum(g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"PROP_CURRENT\": np.nansum(g[\"SUM_CURRENT_BUREAU_BALANCE\"]) / np.nansum(g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"PROP_CLOSED\": np.nansum(g[\"SUM_CLOSED_BUREAU_BALANCE\"]) / np.nansum(g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"PROP_CURRENT_WEIGHTED\": np.nansum(g[\"SUM_CURRENT_BUREAU_BALANCE\"]) / np.nansum(g[\"LEN_BUREAU_BALANCE\"]) / np.nansum(g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MAX_AVG_MONTHS_BALANCE_BUREAU_BALANCE\": np.nanmax(g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MIN_AVG_MONTHS_BALANCE_BUREAU_BALANCE\": np.nanmin(g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"RANGE_AVG_MONTHS_BALANCE_BUREAU_BALANCE\": np.nanmax(g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]) - np.nanmin(g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"SUM_SUM_CURRENT_BUREAU_BALANCE\": np.nansum(g[\"SUM_CURRENT_BUREAU_BALANCE\"]), \n",
    "        \"AVG_PROP_CURRENT\": np.nanmean(g[\"SUM_CURRENT_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"AVG_PROP_DQ\": np.nanmean(g[\"SUM_DQ_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"MAX_PROP_DQ\": np.nanmax(g[\"SUM_DQ_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"]),\n",
    "        \"AVG_PROP_CURRENT_WEIGHTED\": np.nanmean(g[\"SUM_CURRENT_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MIN_PROP_CURRENT_WEIGHTED\": np.nanmin(g[\"SUM_CURRENT_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"AVG_PROP_DQ_WEIGHTED\": np.nanmean(g[\"SUM_DQ_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MAX_PROP_DQ_WEIGHTED\": np.nanmax(g[\"SUM_DQ_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"AVG_PROP_CURRENT_WEIGHTED_AMT\": np.nanmean(g[\"AMT_CREDIT_SUM\"] * g[\"SUM_CURRENT_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MIN_PROP_CURRENT_WEIGHTED_AMT\": np.nanmin(g[\"AMT_CREDIT_SUM\"] * g[\"SUM_CURRENT_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"AVG_PROP_DQ_WEIGHTED_AMT\": np.nanmean(g[\"AMT_CREDIT_SUM\"] * g[\"SUM_DQ_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MAX_PROP_DQ_WEIGHTED_AMT\": np.nanmax(g[\"AMT_CREDIT_SUM\"] * g[\"SUM_DQ_BUREAU_BALANCE\"] / g[\"LEN_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]),          \n",
    "        \"AVG_WORST_DQ_BUREAU_BALANCE\": np.nanmean(g[\"WORST_DQ_BUREAU_BALANCE\"]), \n",
    "        \"MAX_WORST_DQ_BUREAU_BALANCE_WEIGHTED\": np.nanmax(g[\"WORST_DQ_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"AVG_WORST_DQ_BUREAU_BALANCE_WEIGHTED\": np.nanmean(g[\"WORST_DQ_BUREAU_BALANCE\"] / g[\"AVG_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"TOTAL_AMT_CREDIT_SUM_POS_DAYS\": np.nansum(g[\"AMT_CREDIT_SUM\"].where(g[\"DAYS_CREDIT_ENDDATE\"] > 0)),\n",
    "        \"SUM_DAYS_CREDIT_ENDDATE_POS_DAYS\": np.nansum(g[\"DAYS_CREDIT_ENDDATE\"].where(g[\"DAYS_CREDIT_ENDDATE\"] > 0)), \n",
    "        \"MAX_LEN_BUREAU_BALANCE\": np.nanmax(g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"SUM_LEN_BUREAU_BALANCE\": np.nanmax(g[\"LEN_BUREAU_BALANCE\"]), \n",
    "        \"MIN_MIN_MONTHS_BALANCE_BUREAU_BALANCE\": np.nanmin(g[\"MIN_MONTHS_BALANCE_BUREAU_BALANCE\"]), \n",
    "        \"MIN_DAYS_CREDIT_ENDDATE\": np.nanmin(g[\"DAYS_CREDIT_ENDDATE\"]), \n",
    "        \"MAX_DAYS_CREDIT_ENDDATE\": np.nanmax(g[\"DAYS_CREDIT_ENDDATE\"]), \n",
    "        \"SUM_DAYS_CREDIT_ENDDATE\": np.nansum(g[\"DAYS_CREDIT_ENDDATE\"]), \n",
    "        \"SUM_NULL_DAYS_ENDDATE_FACT\": np.nansum(g[\"DAYS_ENDDATE_FACT\"].isnull()), \n",
    "        \"COUNT_BUREAU_RECORDS\": len(g), \n",
    "        \"COUNT_ACTIVE\": np.nansum(g[\"CREDIT_ACTIVE\"] == \"Active\"), \n",
    "        \"MAX_CREDIT_DAY_OVERDUE_WEIGHTED\": np.nanmax(g[\"CREDIT_DAY_OVERDUE\"] / abs(g[\"DAYS_CREDIT_UPDATE\"])), \n",
    "        \"SUM_CREDIT_DAY_OVERDUE_WEIGHTED\": np.nansum(g[\"CREDIT_DAY_OVERDUE\"] / abs(g[\"DAYS_CREDIT_UPDATE\"])), \n",
    "        \"MAX_CREDIT_DAY_OVERDUE\": np.nanmax(g[\"CREDIT_DAY_OVERDUE\"]), \n",
    "        \"SUM_CREDIT_DAY_OVERDUE\": np.nansum(g[\"CREDIT_DAY_OVERDUE\"]), \n",
    "        \"DAYS_SINCE_APPLIED\": - np.nanmax(g[\"DAYS_CREDIT\"]), \n",
    "        \"SUM_INVERSE_DAYS_CREDIT\": - np.nansum(1 / g[\"DAYS_CREDIT\"]), \n",
    "        \"MAX_AMT_CREDIT_MAX_OVERDUE_WEIGHTED\": np.nanmax(g[\"AMT_CREDIT_MAX_OVERDUE\"] / abs(g[\"DAYS_CREDIT_UPDATE\"])), \n",
    "        \"SUM_AMT_CREDIT_MAX_OVERDUE_WEIGHTED\": np.nansum(g[\"AMT_CREDIT_MAX_OVERDUE\"] / abs(g[\"DAYS_CREDIT_UPDATE\"])), \n",
    "        \"MAX_AMT_CREDIT_MAX_OVERDUE\": np.nanmax(g[\"AMT_CREDIT_MAX_OVERDUE\"]), \n",
    "        \"SUM_AMT_CREDIT_MAX_OVERDUE\": np.nansum(g[\"AMT_CREDIT_MAX_OVERDUE\"]), \n",
    "        \"SUM_CNT_CREDIT_PROLONG\": np.nansum(g[\"CNT_CREDIT_PROLONG\"]), \n",
    "        \"SUM_AMT_CREDIT_SUM_DEBT_WEIGHTED\": np.nansum(g[\"AMT_CREDIT_SUM_DEBT\"] / abs(g[\"DAYS_CREDIT_UPDATE\"])), \n",
    "        \"SUM_AMT_CREDIT_SUM_DEBT\": np.nansum(g[\"AMT_CREDIT_SUM_DEBT\"]),\n",
    "        \"BUREAU_UTILIZATION_AVG\": np.nanmean(g[\"AMT_CREDIT_SUM_DEBT\"] / g[\"AMT_CREDIT_SUM_LIMIT\"]), \n",
    "        \"BUREAU_UTILIZATION_MAX\": np.nanmax(g[\"AMT_CREDIT_SUM_DEBT\"] / g[\"AMT_CREDIT_SUM_LIMIT\"]), \n",
    "        \"BUREAU_PROP_SUM_OVERDUE_AVG\": np.nanmean(g[\"AMT_CREDIT_SUM_OVERDUE\"] / g[\"AMT_CREDIT_SUM_DEBT\"]), \n",
    "        \"BUREAU_PROP_MAX_OVERDUE_AVG\": np.nanmean(g[\"AMT_CREDIT_MAX_OVERDUE\"] / g[\"AMT_CREDIT_SUM_DEBT\"]), \n",
    "        \"MAX_DAYS_CREDIT_UPDATE\": np.nanmax(g[\"DAYS_CREDIT_UPDATE\"]), \n",
    "        \"RANGE_DAYS_CREDIT_UPDATE\": np.nanmax(g[\"DAYS_CREDIT_UPDATE\"]) - np.nanmin(g[\"DAYS_CREDIT_UPDATE\"]), \n",
    "        \"DAYS_CREDIT_RANGE\": np.nanmax(g[\"DAYS_CREDIT\"]) - np.nanmin(g[\"DAYS_CREDIT\"]), \n",
    "        \"TOTAL_AMT_CREDIT_SUM_WEIGHTED\": np.nansum(g[\"AMT_CREDIT_SUM\"] / abs(g[\"DAYS_CREDIT_UPDATE\"])),\n",
    "        \"TOTAL_AMT_CREDIT_SUM\": np.nansum(g[\"AMT_CREDIT_SUM\"]),\n",
    "        \"COUNT_CREDIT_CARD\": np.nansum(g[\"CREDIT_TYPE\"] == \"Credit card\"), \n",
    "        \"COUNT_CAR_LOAN\": np.nansum(g[\"CREDIT_TYPE\"] == \"Car loan\"), \n",
    "        \"COUNT_MORTGAGE\": np.nansum(g[\"CREDIT_TYPE\"] == \"Mortgage\"), \n",
    "        \"SUM_AMT_ANNUITY\": np.nansum(g[\"AMT_ANNUITY\"])}\n",
    "    \n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = pd.read_csv(path + \"bureau.csv\")\n",
    "bureau_balance_agg = pd.read_csv(path + \"bureau_balance_agg.csv\")\n",
    "\n",
    "bureau_joined = pd.merge(bureau, \n",
    "                         bureau_balance_agg, \n",
    "                         how=\"left\", \n",
    "                         on=\"SK_ID_BUREAU\")\n",
    "\n",
    "bureau_agg = bureau_joined.groupby(\"SK_ID_CURR\").apply(bureau_agg_func).reset_index()\n",
    "bureau_agg.to_csv(path + \"bureau_agg.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation function\n",
    "\n",
    "* CNT_INSTALMENT_MATURE_CUM, cumulative paid installments (positive balance while this is flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_agg_func(g):\n",
    "    mask6 = g[\"MONTHS_BALANCE\"] >= -6\n",
    "    mask12 = g[\"MONTHS_BALANCE\"] >= -12\n",
    "    \n",
    "    d = {\"MAX_CREDIT_CARD_SK_DPD_6M\": np.nanmax(g[\"SK_DPD\"].where(mask6)), \n",
    "        \"MAX_CREDIT_CARD_SK_DPD_12M\": np.nanmax(g[\"SK_DPD\"].where(mask6)),\n",
    "        \"MAX_AMT_DRAWINGS_CURRENT_6M\": np.nanmax(g[\"AMT_DRAWINGS_CURRENT\"].where(mask6)), \n",
    "        \"MAX_AMT_DRAWINGS_CURRENT_12M\": np.nanmax(g[\"AMT_DRAWINGS_CURRENT\"].where(mask12)), \n",
    "        \"MAX_AMT_INST_MIN_REGULARITY_6M\": np.nanmax(g[\"AMT_INST_MIN_REGULARITY\"].where(mask6)), \n",
    "        \"MAX_AMT_INST_MIN_REGULARITY_12M\": np.nanmax(g[\"AMT_INST_MIN_REGULARITY\"].where(mask12)), \n",
    "        \"MAX_CNT_DRAWINGS_POS_CURRENT_6M\": np.nanmax(g[\"CNT_DRAWINGS_POS_CURRENT\"].where(mask6)), \n",
    "        \"MAX_CNT_DRAWINGS_POS_CURRENT_12M\": np.nanmax(g[\"CNT_DRAWINGS_POS_CURRENT\"].where(mask12)),  \n",
    "        \n",
    "        \n",
    "        \"DIFF_AVG_BALANCE_6M_12M\": np.nanmean(g[\"AMT_BALANCE\"].where(mask6)) - np.nanmean(g[\"AMT_BALANCE\"].where(mask6 ^ mask12)),\n",
    "        \"AVG_BALANCE_6M\": np.nanmean(g[\"AMT_BALANCE\"].where(mask6)),\n",
    "        \"AVG_UTILIZATION_6M\": np.nanmean((g[\"AMT_BALANCE\"] / g[\"AMT_CREDIT_LIMIT_ACTUAL\"]).where(mask6)),\n",
    "        \"AVG_BALANCE\": np.nanmean(g[\"AMT_BALANCE\"]), \n",
    "        \"MAX_BALANCE\": np.nanmax(g[\"AMT_BALANCE\"]), \n",
    "        \"SUM_BALANCE\": np.nansum(g[\"AMT_BALANCE\"]), \n",
    "        \"MAX_MONTHS_BALANCE\": np.nanmax(abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"MIN_MONTHS_BALANCE\": np.nanmin(abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"RANGE_MONTHS_BALANCE\": np.nanmax(g[\"MONTHS_BALANCE\"]) - np.nanmin(g[\"MONTHS_BALANCE\"]), \n",
    "        \"AVG_UTILIZATION\": np.nanmean(g[\"AMT_BALANCE\"] / g[\"AMT_CREDIT_LIMIT_ACTUAL\"]), \n",
    "        \"MAX_UTILIZATION\": np.nanmax(g[\"AMT_BALANCE\"] / g[\"AMT_CREDIT_LIMIT_ACTUAL\"]), \n",
    "        \"AVG_BALANCE_WEIGHTED\": np.nanmean(g[\"AMT_BALANCE\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"MAX_BALANCE_WEIGHTED\": np.nanmax(g[\"AMT_BALANCE\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"SUM_BALANCE_WEIGHTED\": np.nansum(g[\"AMT_BALANCE\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"AVG_UTILIZATION_WEIGHTED\": np.nanmean(g[\"AMT_BALANCE\"] / g[\"AMT_CREDIT_LIMIT_ACTUAL\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"MAX_UTILIZATION_WEIGHTED\": np.nanmax(g[\"AMT_BALANCE\"] / g[\"AMT_CREDIT_LIMIT_ACTUAL\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"MAX_DPD_WEIGHTED\": np.nanmax(g[\"SK_DPD\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"MAX_DPD_DEF_WEIGHTED\": np.nanmax(g[\"SK_DPD_DEF\"] / abs(g[\"MONTHS_BALANCE\"])), \n",
    "        \"SUM_CNT_DRAWINGS_CURRENT\": np.nansum(g[\"CNT_DRAWINGS_CURRENT\"]), \n",
    "        \"AVG_CNT_DRAWINGS_CURRENT\": np.nanmean(g[\"CNT_DRAWINGS_CURRENT\"]), \n",
    "        \"MAX_CNT_DRAWINGS_CURRENT\": np.nanmax(g[\"CNT_DRAWINGS_CURRENT\"]), \n",
    "        \"SUM_AMT_DRAWINGS_CURRENT\": np.nansum(g[\"AMT_DRAWINGS_CURRENT\"]), \n",
    "        \"AVG_AMT_DRAWINGS_CURRENT\": np.nanmean(g[\"AMT_DRAWINGS_CURRENT\"]), \n",
    "        \"MAX_AMT_DRAWINGS_CURRENT\": np.nanmax(g[\"AMT_DRAWINGS_CURRENT\"]), \n",
    "        \"MIN_AMT_PAYMENT_CURRENT_DIV_AMT_INST_MIN_REGULARITY\": np.nanmin(g[\"AMT_PAYMENT_CURRENT\"] / g[\"AMT_INST_MIN_REGULARITY\"]), \n",
    "        \"AVG_AMT_PAYMENT_CURRENT_DIV_AMT_INST_MIN_REGULARITY\": np.nanmean(g[\"AMT_PAYMENT_CURRENT\"] / g[\"AMT_INST_MIN_REGULARITY\"]), \n",
    "        \"MAX_AMT_PAYMENT_CURRENT_DIV_AMT_INST_MIN_REGULARITY\": np.nanmax(g[\"AMT_PAYMENT_CURRENT\"] / g[\"AMT_INST_MIN_REGULARITY\"])}\n",
    "    \n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card = pd.read_csv(path + \"credit_card_balance.csv\")\n",
    "\n",
    "credit_card_agg = credit_card.groupby(\"SK_ID_CURR\").apply(credit_card_agg_func).reset_index()\n",
    "credit_card_agg.to_csv(path + \"credit_card_agg.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installment_agg_func(g):\n",
    "    mask6 = g[\"DAYS_ENTRY_PAYMENT\"] >= -180\n",
    "    mask12 = g[\"DAYS_ENTRY_PAYMENT\"] >= -360\n",
    "    \n",
    "    d = {\"SUM_UNDERPAYMENT_12M\": np.nansum((g[\"AMT_INSTALMENT\"] - g[\"AMT_PAYMENT\"]).where(mask12)), \n",
    "        \"SUM_UNDERPAYMENT_6M\": np.nansum((g[\"AMT_INSTALMENT\"] - g[\"AMT_PAYMENT\"]).where(mask6)), \n",
    "        \"MAX_PAYMENT_SIZE_6M\": np.nanmax(g[\"AMT_PAYMENT\"].where(mask6)), \n",
    "        \"MAX_PAYMENT_SIZE_12M\": np.nanmax(g[\"AMT_PAYMENT\"].where(mask12)), \n",
    "        \"MIN_PAYMENT_SIZE_6M\": np.nanmin(g[\"AMT_PAYMENT\"].where(mask6)),\n",
    "        \"MAX_ABS_DAYS_INSTALMENT\": np.nanmax(abs(g[\"DAYS_INSTALMENT\"])), \n",
    "        \"COUNT_UNDERPAYMENT\": np.nansum(g[\"AMT_PAYMENT\"] / g[\"AMT_INSTALMENT\"] < 0.5), \n",
    "        \"SUM_UNDERPAYMENT\": np.nansum(g[\"AMT_INSTALMENT\"] - g[\"AMT_PAYMENT\"]), \n",
    "        \"SUM_UNDERPAYMENT_WEIGHTED\": np.nansum((g[\"AMT_INSTALMENT\"] - g[\"AMT_PAYMENT\"]) / abs(g[\"DAYS_ENTRY_PAYMENT\"])), \n",
    "        \"MAX_UNDERPAYMENT\": np.nanmax(g[\"AMT_INSTALMENT\"] - g[\"AMT_PAYMENT\"]), \n",
    "        \"AVG_PAYMENT_SIZE_WEIGHTED\": np.nanmean(g[\"AMT_PAYMENT\"] / abs(g[\"DAYS_ENTRY_PAYMENT\"])), \n",
    "        \"AVG_PAYMENT_SIZE\": np.nanmean(g[\"AMT_PAYMENT\"]), \n",
    "        \"MAX_PAYMENT_SIZE_WEIGHTED\": np.nanmax(g[\"AMT_PAYMENT\"] / abs(g[\"DAYS_ENTRY_PAYMENT\"])), \n",
    "        \"MAX_PAYMENT_SIZE\": np.nanmax(g[\"AMT_PAYMENT\"]), \n",
    "        \"MIN_PAYMENT_SIZE_WEIGHTED\": np.nanmin(g[\"AMT_PAYMENT\"] / abs(g[\"DAYS_ENTRY_PAYMENT\"])), \n",
    "        \"MIN_PAYMENT_SIZE\": np.nanmin(g[\"AMT_PAYMENT\"]),\n",
    "        \"SUM_PAYMENT_WEIGHTED\": np.nansum(g[\"AMT_PAYMENT\"] / abs(g[\"DAYS_ENTRY_PAYMENT\"])), \n",
    "        \"SUM_PAYMENT\": np.nansum(g[\"AMT_PAYMENT\"]),\n",
    "        \"SUM_DAYS_ENTRY_PAYMENT_GT_DAYS_INSTALMENT\": np.nansum(g[\"DAYS_ENTRY_PAYMENT\"] > g[\"DAYS_INSTALMENT\"]), \n",
    "        \"MAX_DAYS_ENTRY_PAYMENT\": np.nanmax(g[\"DAYS_ENTRY_PAYMENT\"]), \n",
    "        \"MIN_DAYS_ENTRY_PAYMENT\": np.nanmin(g[\"DAYS_ENTRY_PAYMENT\"]), \n",
    "        \"RANGE_DAYS_ENTRY_PAYMENT\": np.nanmax(g[\"DAYS_ENTRY_PAYMENT\"]) - np.nanmin(g[\"DAYS_ENTRY_PAYMENT\"])}\n",
    "    \n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments = pd.read_csv(path + \"installments_payments.csv\")\n",
    "\n",
    "installment_agg = installments.groupby(\"SK_ID_CURR\").apply(installment_agg_func).reset_index()\n",
    "installment_agg.to_csv(path + \"installment_agg.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point of Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash_agg_func(g):\n",
    "    d = {\"MAX_POS_DPD\": np.nanmax(g[\"SK_DPD\"]), \n",
    "        \"MAX_POS_DPD_DEF\": np.nanmax(g[\"SK_DPD_DEF\"]), \n",
    "        \"NUM_POS_CASH\": g[\"SK_ID_PREV\"].nunique()}\n",
    "    \n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash = pd.read_csv(path + \"POS_CASH_balance.csv\")\n",
    "\n",
    "pos_cash_agg = pos_cash.groupby(\"SK_ID_CURR\").apply(pos_cash_agg_func).reset_index()\n",
    "pos_cash_agg.to_csv(path + \"pos_cash_agg.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application = pd.read_csv(path + \"application_\" + train_or_test + \".csv\")\n",
    "previous_agg = pd.read_csv(path + \"previous_agg.csv\")\n",
    "# bureau_balance_agg should already be joined with bureau_agg\n",
    "bureau_agg = pd.read_csv(path + \"bureau_agg.csv\")\n",
    "credit_card_agg = pd.read_csv(path + \"credit_card_agg.csv\")\n",
    "installment_agg = pd.read_csv(path + \"installment_agg.csv\")\n",
    "pos_cash_agg = pd.read_csv(path + \"pos_cash_agg.csv\")\n",
    "\n",
    "df = pd.merge(application, previous_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = pd.merge(df, bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = pd.merge(df, credit_card_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = pd.merge(df, installment_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "df = pd.merge(df, pos_cash_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "del previous_agg, bureau_agg, credit_card_agg, installment_agg, pos_cash_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TOTAL_AMT_CREDIT_SUM_DIV_SUM_DAYS_CREDIT_ENDDATE\"] = df[\"TOTAL_AMT_CREDIT_SUM\"] / df[\"SUM_DAYS_CREDIT_ENDDATE\"]\n",
    "df[\"TOTAL_AMT_CREDIT_SUM_POS_DAYS_DIV_SUM_DAYS_CREDIT_ENDDATE_POS_DAYS\"] = df[\"TOTAL_AMT_CREDIT_SUM_POS_DAYS\"] / df[\"SUM_DAYS_CREDIT_ENDDATE_POS_DAYS\"]\n",
    "df[\"MAX_ABS_DAYS_INSTALMENT_DIV_DAYS_BIRTH\"] = df[\"MAX_ABS_DAYS_INSTALMENT\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"FLAG_OWN_CAR\"] = (df[\"FLAG_OWN_CAR\"] == \"Y\").astype(int)\n",
    "df[\"FLAG_OWN_REALTY\"] = (df[\"FLAG_OWN_REALTY\"] == \"Y\").astype(int)\n",
    "df[\"AMT_CREDIT_DIV_AMT_INCOME_TOTAL\"] = df[\"AMT_CREDIT\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "df[\"AMT_CREDIT_DIV_AMT_GOODS_PRICE\"] = df[\"AMT_CREDIT\"] / df[\"AMT_GOODS_PRICE\"]\n",
    "df[\"AMT_CREDIT_DIV_SUM_PAYMENT\"] = df[\"AMT_CREDIT\"] / df[\"SUM_PAYMENT\"]\n",
    "df[\"AMT_GOODS_PRICE_DIV_AMT_INCOME_TOTAL\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "df[\"AMT_CREDIT_DIV_AMT_ANNUITY\"] = df[\"AMT_CREDIT\"] / df[\"AMT_ANNUITY\"]\n",
    "df[\"AMT_CREDIT_DIV_AVG_PREV_REQ_AMOUNT\"] = df[\"AMT_CREDIT\"] / df[\"AVG_PREV_REQ_AMOUNT\"]\n",
    "df[\"AMT_CREDIT_DIV_MAX_PREV_REQ_AMOUNT\"] = df[\"AMT_CREDIT\"] / df[\"MAX_PREV_REQ_AMOUNT\"]\n",
    "df[\"EXT_SOURCE_PROD\"] = df[\"EXT_SOURCE_1\"] * df[\"EXT_SOURCE_2\"] * df[\"EXT_SOURCE_3\"]\n",
    "df[\"DAYS_EMPLOYED_DIV_DAYS_BIRTH\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"AVG_PAYMENT_SIZE_DIV_AMT_INCOME_TOTAL\"] = df[\"AVG_PAYMENT_SIZE\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "df[\"AVG_PAYMENT_SIZE_DIV_AMT_CREDIT\"] = df[\"AVG_PAYMENT_SIZE\"] / df[\"AMT_CREDIT\"]\n",
    "df[\"AVG_PAYMENT_SIZE_DIV_AMT_ANNUITY\"] = df[\"AVG_PAYMENT_SIZE\"] / df[\"AMT_ANNUITY\"]\n",
    "df[\"DAYS_REGISTRATION_PLUS_DAYS_ID_PUBLISH\"] = df[\"DAYS_REGISTRATION\"] + df[\"DAYS_ID_PUBLISH\"]\n",
    "df[\"SUM_REFUSED_CONTRACT_DIV_SUM_APPR_CONTRACT\"] = df[\"SUM_REFUSED_CONTRACT\"] / df[\"SUM_APPR_CONTRACT\"]\n",
    "df[\"MAX_UTILIZATION_DIV_AVG_UTILIZATION\"] = df[\"MAX_UTILIZATION\"] / df[\"AVG_UTILIZATION\"]\n",
    "df[\"MAX_PREV_REQ_AMOUNT_DIV_AMT_CREDIT\"] = df[\"MAX_PREV_REQ_AMOUNT\"] / df[\"AMT_CREDIT\"]\n",
    "df[\"AMT_INCOME_TOTAL_DIV_DAYS_BIRTH\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"SUM_DAYS_ID_REG_PHONE\"] = df[\"DAYS_ID_PUBLISH\"] + df[\"DAYS_REGISTRATION\"] + df[\"DAYS_LAST_PHONE_CHANGE\"]\n",
    "df[\"SUM_REQ_CREDIT\"] = df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] + df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] + df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"] + df[\"AMT_REQ_CREDIT_BUREAU_MON\"] + df[\"AMT_REQ_CREDIT_BUREAU_QRT\"] + df[\"AMT_REQ_CREDIT_BUREAU_YEAR\"]\n",
    "df[\"DEF_30_PLUS_60_CNT_SOCIAL_CIRCLE\"] = df[\"DEF_30_CNT_SOCIAL_CIRCLE\"] + df[\"DEF_60_CNT_SOCIAL_CIRCLE\"]\n",
    "df[\"OWN_CAR_AGE_DIV_DAYS_BIRTH\"] = df[\"OWN_CAR_AGE\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"LANDAREA_DIV_TOTALAREA_MODE\"] = df[\"LANDAREA_MODE\"] / df[\"TOTALAREA_MODE\"]\n",
    "df[\"OWN_CAR_AGE_PLUS_DAYS_BIRTH\"] = df[\"OWN_CAR_AGE\"] + df[\"DAYS_BIRTH\"]\n",
    "df[\"AMT_ANNUITY_DIV_DAYS_BIRTH\"] = df[\"AMT_ANNUITY\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"AMT_ANNUITY_DIV_DAYS_EMPLOYED\"] = df[\"AMT_ANNUITY\"] / df[\"DAYS_EMPLOYED\"]\n",
    "df[\"AMT_ANNUITY_PROD_DAYS_EMPLOYED\"] = df[\"AMT_ANNUITY\"] * df[\"DAYS_EMPLOYED\"]\n",
    "df[\"DAYS_REGISTRATION_DIV_DAYS_ID_PUBLISH\"] = df[\"DAYS_REGISTRATION\"] / df[\"DAYS_ID_PUBLISH\"]\n",
    "df[\"DAYS_REGISTRATION_DIV_DAYS_LAST_PHONE_CHANGE\"] = df[\"DAYS_REGISTRATION\"] / df[\"DAYS_LAST_PHONE_CHANGE\"]\n",
    "df[\"REGION_RATING_CLIENT_W_CITY_DIV_REGION_POPULATION_RELATIVE\"] = df[\"REGION_RATING_CLIENT_W_CITY\"] / df[\"REGION_POPULATION_RELATIVE\"]\n",
    "df[\"REGION_RATING_CLIENT_W_CITY_DIV_REGION_POPULATION_RELATIVE\"] = df[\"REGION_RATING_CLIENT_W_CITY\"] * df[\"REGION_POPULATION_RELATIVE\"]\n",
    "df[\"SUM_REG_NOT_FLAG\"] = df[\"REG_REGION_NOT_LIVE_REGION\"] + df[\"REG_REGION_NOT_WORK_REGION\"] + df[\"LIVE_REGION_NOT_WORK_REGION\"] + df[\"REG_CITY_NOT_LIVE_CITY\"] + df[\"REG_CITY_NOT_WORK_CITY\"] + df[\"LIVE_CITY_NOT_WORK_CITY\"]\n",
    "df[\"SUM_AVG_BUILD\"] = df[\"APARTMENTS_AVG\"] + df[\"BASEMENTAREA_AVG\"] + df[\"YEARS_BEGINEXPLUATATION_AVG\"] + df[\"YEARS_BUILD_AVG\"] + df[\"COMMONAREA_AVG\"] + df[\"ELEVATORS_AVG\"] + df[\"ENTRANCES_AVG\"] + df[\"FLOORSMAX_AVG\"] + df[\"FLOORSMIN_AVG\"] + df[\"LANDAREA_AVG\"] + df[\"LIVINGAPARTMENTS_AVG\"] + df[\"LIVINGAREA_AVG\"] + df[\"NONLIVINGAPARTMENTS_AVG\"] + df[\"NONLIVINGAREA_AVG\"]\n",
    "df[\"SUM_MODE_BUILD\"] = df[\"APARTMENTS_MODE\"] + df[\"BASEMENTAREA_MODE\"] + df[\"YEARS_BEGINEXPLUATATION_MODE\"] + df[\"YEARS_BUILD_MODE\"] + df[\"COMMONAREA_MODE\"] + df[\"ELEVATORS_MODE\"] + df[\"ENTRANCES_MODE\"] + df[\"FLOORSMAX_MODE\"] + df[\"FLOORSMIN_MODE\"] + df[\"LANDAREA_MODE\"] + df[\"LIVINGAPARTMENTS_MODE\"] + df[\"LIVINGAREA_MODE\"] + df[\"NONLIVINGAPARTMENTS_MODE\"] + df[\"NONLIVINGAREA_MODE\"]\n",
    "df[\"SUM_MEDI_BUILD\"] = df[\"APARTMENTS_MEDI\"] + df[\"BASEMENTAREA_MEDI\"] + df[\"YEARS_BEGINEXPLUATATION_MEDI\"] + df[\"YEARS_BUILD_MEDI\"] + df[\"COMMONAREA_MEDI\"] + df[\"ELEVATORS_MEDI\"] + df[\"ENTRANCES_MEDI\"] + df[\"FLOORSMAX_MEDI\"] + df[\"FLOORSMIN_MEDI\"] + df[\"LANDAREA_MEDI\"] + df[\"LIVINGAPARTMENTS_MEDI\"] + df[\"LIVINGAREA_MEDI\"] + df[\"NONLIVINGAPARTMENTS_MEDI\"] + df[\"NONLIVINGAREA_MEDI\"]\n",
    "df[\"SUM_DOC_FLAG\"] = df[\"FLAG_DOCUMENT_2\"] + df[\"FLAG_DOCUMENT_3\"] + df[\"FLAG_DOCUMENT_4\"] + df[\"FLAG_DOCUMENT_5\"] + df[\"FLAG_DOCUMENT_6\"] + df[\"FLAG_DOCUMENT_7\"] + df[\"FLAG_DOCUMENT_8\"] + df[\"FLAG_DOCUMENT_9\"] + df[\"FLAG_DOCUMENT_10\"] + df[\"FLAG_DOCUMENT_11\"] + df[\"FLAG_DOCUMENT_12\"] + df[\"FLAG_DOCUMENT_13\"] + df[\"FLAG_DOCUMENT_14\"] + df[\"FLAG_DOCUMENT_15\"] + df[\"FLAG_DOCUMENT_16\"] + df[\"FLAG_DOCUMENT_17\"] + df[\"FLAG_DOCUMENT_18\"] + df[\"FLAG_DOCUMENT_19\"] + df[\"FLAG_DOCUMENT_20\"] + df[\"FLAG_DOCUMENT_21\"]\n",
    "df[\"CNT_CHILDREN_DIV_DAYS_BIRTH\"] = df[\"CNT_CHILDREN\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"CNT_CHILDREN_DIV_REGION_POPULATION_RELATIVE\"] = df[\"CNT_CHILDREN\"] / df[\"REGION_POPULATION_RELATIVE\"]\n",
    "df[\"FLAG_OWN_REALTY_PROD_REGION_POPULATION_RELATIVE\"] = df[\"FLAG_OWN_REALTY\"] * df[\"REGION_POPULATION_RELATIVE\"]\n",
    "df[\"FLAG_OWN_REALTY_DIV_REGION_POPULATION_RELATIVE\"] = df[\"FLAG_OWN_REALTY\"] / df[\"REGION_POPULATION_RELATIVE\"]\n",
    "df[\"FLAG_OWN_CAR_DIV_OWN_CAR_AGE\"] = df[\"FLAG_OWN_CAR\"] / df[\"OWN_CAR_AGE\"]\n",
    "df[\"EXT_SOURCE_1_DIV_DAYS_BIRTH\"] = df[\"EXT_SOURCE_1\"] / df[\"DAYS_BIRTH\"]\n",
    "df[\"EXT_SOURCE_1_PROD_DAYS_BIRTH\"] = df[\"EXT_SOURCE_1\"] * df[\"DAYS_BIRTH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove infinite values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([-np.inf, np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove income outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"AMT_INCOME_TOTAL\"] > 500000, \"AMT_INCOME_TOTAL\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle special values for DAYS_EMPLOYED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"DAYS_EMPLOYED\"] > 0, \"DAYS_EMPLOYED\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, dummy_na=True)\n",
    "df.columns = df.columns.str.replace(\"\\s+\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write preprocessed data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path + \"_preprocessed_\" + train_or_test + \".csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
